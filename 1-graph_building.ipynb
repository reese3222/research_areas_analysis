{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Areas Analysis Using Graph Data and Large Language Models\n",
    "\n",
    "### Donato Riccio\n",
    "\n",
    "![](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph construction\n",
    "\n",
    "The following code loads the dataset from csv and builds a graph using networkx. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [03:52,  1.30it/s]                              \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 10000  \n",
    "\n",
    "# This is done in a streaming way to avoid loading the entire dataset into memory along with the graph\n",
    "\n",
    "for chunk in tqdm(pd.read_csv('data/dblp_ml.csv', chunksize=chunk_size), total=2941588/chunk_size):\n",
    "    \n",
    "    # Process each row in the chunk\n",
    "    for idx, row in chunk.iterrows():\n",
    "        paper_id = row['id']\n",
    "        references = str(row['references']).split(';') if pd.notna(row['references']) else []\n",
    "\n",
    "        # Add paper node\n",
    "        G.add_node(paper_id, type='paper')\n",
    "\n",
    "        # Add reference edges (directed)\n",
    "        for ref in references:\n",
    "            if ref and ref.lower() != 'nan':\n",
    "                ref = int(ref)  # Ensure the reference ID is an integer\n",
    "                G.add_edge(paper_id, ref, relationship='cites')  # Directed edge\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running InfoMap to partition the graph in communities\n",
    "\n",
    "The algorithm was chosen for two reasons:\n",
    "- It's suited for directed graphs\n",
    "- It's very time efficient (O(nlogn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34170947/34170947 [02:57<00:00, 192204.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "  Infomap v2.7.1 starts at 2024-06-12 18:44:03\n",
      "  -> Input network: \n",
      "  -> No file output!\n",
      "=======================================================\n",
      "  -> Ordinary network input, using the Map Equation for first order network flows\n",
      "Calculating global network flow using flow model 'undirected'... \n",
      "  -> Using undirected links.\n",
      "  => Sum node flow: 1, sum link flow: 1\n",
      "Build internal network with 2880605 nodes and 34170947 links...\n",
      "  -> One-level codelength: 20.4467932\n",
      "\n",
      "================================================\n",
      "Trial 1/1 starting at 2024-06-12 18:46:36\n",
      "================================================\n",
      "Two-level compression: 20% 3.7% 0.0486511818% 0.0481414097% 0.00442233328% 0.00835782759% \n",
      "Partitioned to codelength 4.68737479 + 11.0540549 = 15.74142964 in 24029 (24028 non-trivial) modules.\n",
      "Super-level compression: 2.48449717% to codelength 15.35033427 in 1808 top modules.\n",
      "\n",
      "Recursive sub-structure compression: 14.6931561% 0.0781793798% 5.21006099e-07% 0% . Found 5 levels with codelength 15.21972421\n",
      "\n",
      "=> Trial 1/1 finished in 461.651345s with codelength 15.2197242\n",
      "\n",
      "\n",
      "================================================\n",
      "Summary after 1 trial\n",
      "================================================\n",
      "Best end modular solution in 5 levels:\n",
      "Per level number of modules:         [       1808,       46193,        9417,           7,           0] (sum: 57425)\n",
      "Per level number of leaf nodes:      [          0,        3978,     2684594,      191987,          46] (sum: 2880605)\n",
      "Per level average child degree:      [       1808,     27.7494,     58.3208,      20.388,     6.57143] (average: 56.3958)\n",
      "Per level codelength for modules:    [0.811485347, 4.595150592, 0.185471802, 0.000001897, 0.000000000] (sum: 5.592109637)\n",
      "Per level codelength for leaf nodes: [0.000000000, 0.000094699, 9.203235743, 0.424273239, 0.000010890] (sum: 9.627614572)\n",
      "Per level codelength total:          [0.811485347, 4.595245291, 9.388707545, 0.424275136, 0.000010890] (sum: 15.219724208)\n",
      "\n",
      "===================================================\n",
      "  Infomap ends at 2024-06-12 18:54:18\n",
      "  (Elapsed time: 10m 15.037s)\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2880605/2880605 [00:28<00:00, 101496.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities detected: 1808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from infomap import Infomap\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Infomap\n",
    "im = Infomap()\n",
    "\n",
    "# Add edges to the Infomap object\n",
    "for u, v, data in tqdm(G.edges(data=True)):\n",
    "    im.addLink(u, v)\n",
    "\n",
    "# Run Infomap\n",
    "im.run()\n",
    "\n",
    "# Retrieve the communities\n",
    "communities = im.getModules()\n",
    "import pandas as pd\n",
    "# Prepare a DataFrame to store the results\n",
    "processed_papers_df = pd.DataFrame(\n",
    "    [(node, communities[node]) for node in communities],\n",
    "    columns=['node_id', 'community']\n",
    ")\n",
    "\n",
    "# add community to the graph\n",
    "for row in tqdm(processed_papers_df.itertuples(), total=len(processed_papers_df)):\n",
    "    G.nodes[row.node_id]['community'] = row.community\n",
    "\n",
    "print(f\"Number of communities detected: {im.numTopModules()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating HITS scores\t\n",
    "    \n",
    "HITS provides two scores for each node: a hub score and an authority score. In the context of a citation network, a high authority score indicates a highly cited paper (important and influential in its field), whereas a high hub score indicates a paper that cites many important papers. This dual scoring system allows for a nuanced understanding of a paper’s role in the network, distinguishing between sources of information and distributors of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Calculate HITS scores\n",
    "hits = nx.hits(G)\n",
    "\n",
    "#hits df with node_id, hub_score, authority_score\n",
    "hits_data = []\n",
    "\n",
    "# Update the node attributes with the HITS scores\n",
    "for node in tqdm(G.nodes):\n",
    "    hits_data.append((node, hits[0][node], hits[1][node]))\n",
    "    G.nodes[node]['hub_score'] = hits[0][node]\n",
    "    G.nodes[node]['authority_score'] = hits[1][node]\n",
    "\n",
    "hits_df = pd.DataFrame(hits_data, columns=['node_id', 'hub_score', 'authority_score'])\n",
    "\n",
    "#join with the processed_papers_df\n",
    "processed_papers_df = pd.merge(hits_df, processed_papers_df, on='node_id')\n",
    "\n",
    "\n",
    "\n",
    "processed_papers_df.to_csv('data/processed_papers_df.csv', index=False)\n",
    "\n",
    "# Save the updated graph to a pickle file for faster loading\n",
    "with open('data/graphs/dblp_ml_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "\n",
    "processed_papers_df.to_csv('data/processed_papers_df.csv', index=False)\n",
    "\n",
    "# Save the updated graph to a pickle file for faster loading\n",
    "with open('data/graphs/dblp_ml_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
