{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Research Areas Analysis Using Graph Data and Large Language Models\n","\n","### Donato Riccio\n","\n","![](image-1.png)"]},{"cell_type":"markdown","metadata":{},"source":["## Converting JSON to CSV\n","The first step for building the dataset is converting JSON to CSV. We'll use the CSV for the graph as it's easier to work with. We only need id, references and keyword for this step.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T10:23:04.052762Z","iopub.status.busy":"2023-06-23T10:23:04.052185Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["100000:4.12s 200000:8.16s 300000:12.95s 400000:18.38s 500000:23.82s 600000:29.24s 700000:35.02s 800000:41.09s 900000:47.07s 1000000:53.0s 1100000:59.03s 1200000:64.96s 1300000:71.05s 1400000:77.05s 1500000:83.06s 1600000:88.89s 1700000:94.84s 1800000:100.7s 1900000:106.7s 2000000:112.67s 2100000:118.89s 2200000:125.19s 2300000:131.28s 2400000:137.45s 2500000:143.49s 2600000:149.73s 2700000:155.91s 2800000:161.98s 2900000:168.06s 3000000:173.87s 3100000:179.52s 3200000:184.07s 3300000:189.62s 3400000:195.65s 3500000:201.0s 3600000:206.73s 3700000:212.46s 3800000:218.28s 3900000:224.29s 4000000:230.19s 4100000:236.13s 4200000:241.75s 4300000:247.18s 4400000:252.33s 4500000:256.31s 4600000:259.92s 4700000:265.31s 4800000:270.49s "]}],"source":["# Code from: https://www.kaggle.com/code/shreyasbhatk/csv-conversion-all-fields\n","\n","import ijson\n","import time\n","import csv\n","import numpy as np\n","from tqdm import tqdm\n","from decimal import Decimal\n","\n","start = time.process_time()\n","\n","PAPER = []\n","Author = []\n","count = 0\n","\n","with open('data/dblp.v12.json', \"rb\") as f, open(\"data/dblp.csv\", \"w\", newline=\"\") as csvfile:\n","    fieldnames = ['id', 'references', 'keyword']\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    for i, element in enumerate(ijson.items(f, \"item\")):\n","        paper = {}\n","        paper['id'] = element['id']\n","\n","        references = element.get('references')\n","        if references:\n","            paper['references'] = ';'.join(str(int(r)) for r in references)\n","        else:\n","            paper['references'] = np.nan\n","\n","        fos = element.get('fos')\n","        if fos:\n","            fosunparsed = element['fos']\n","            keyword = []\n","           # weight = []\n","\n","            for i in fosunparsed:\n","                if isinstance(i['w'], (int, float, Decimal)):\n","                    keyword.append(str(i['name']))  # Convert to string\n","                else:\n","                    keyword.append(str(np.nan))  # Convert to string\n","\n","        else:\n","            keyword = []\n","            weight = []\n","\n","        paper['keyword'] = ';'.join(keyword)\n","\n","  \n","        count += 1\n","        writer.writerow(paper)\n","\n","        if count % 100000 == 0:\n","            print(f\"{count}:{round((time.process_time() - start), 2)}s \", end=\"\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Extracting Machine Learning Papers and Their References from DBLP Dataset\n","\n","We aim to extract papers related to machine learning (ML) and similar research areads from the DBLP dataset, along with their references. The process involves several steps:\n","\n","1. **Filter ML-related Papers**: We begin by filtering the papers in the DBLP dataset that are related to machine learning, deep learning, artificial intelligence, or neural networks based on their keywords.\n","\n","2. **Identify Missing References**: For each selected ML paper, we extract the references. Since ML papers can cite papers outside the filtered subset, we need to add the missing references to avoid having missing papers in the graph.\n","\n","3. **Iteratively Add Missing Papers**: We iteratively add these missing referenced papers to our dataset. This process ensures that all references are included, even if they were not initially classified under ML-related keywords. This process is iterated until no more missing papers are found.\n","\n","4. **Save the Final Dataset**: The final dataset, which now includes all ML-related papers and their references, is saved as a CSV file for further analysis.\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["1283069it [00:16, 76414.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 829088\n","Added paper ids: 829088\n"]},{"name":"stderr","output_type":"stream","text":["829088it [00:11, 72839.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 594257\n","Added paper ids: 594257\n"]},{"name":"stderr","output_type":"stream","text":["594257it [00:07, 75440.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 227768\n","Added paper ids: 227768\n"]},{"name":"stderr","output_type":"stream","text":["227768it [00:03, 74131.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 67088\n","Added paper ids: 67088\n"]},{"name":"stderr","output_type":"stream","text":["67088it [00:00, 73957.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 18977\n","Added paper ids: 18977\n"]},{"name":"stderr","output_type":"stream","text":["18977it [00:00, 75035.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 5139\n","Added paper ids: 5139\n"]},{"name":"stderr","output_type":"stream","text":["5139it [00:00, 72674.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 1563\n","Added paper ids: 1563\n"]},{"name":"stderr","output_type":"stream","text":["1563it [00:00, 71090.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 527\n","Added paper ids: 527\n"]},{"name":"stderr","output_type":"stream","text":["527it [00:00, 7690.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 169\n","Added paper ids: 169\n"]},{"name":"stderr","output_type":"stream","text":["169it [00:00, 63629.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 71\n","Added paper ids: 71\n"]},{"name":"stderr","output_type":"stream","text":["71it [00:00, 55808.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 40\n","Added paper ids: 40\n"]},{"name":"stderr","output_type":"stream","text":["40it [00:00, 61840.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 20\n","Added paper ids: 20\n"]},{"name":"stderr","output_type":"stream","text":["20it [00:00, 49490.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 2\n","Added paper ids: 2\n"]},{"name":"stderr","output_type":"stream","text":["2it [00:00, 12905.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 2\n","Added paper ids: 2\n"]},{"name":"stderr","output_type":"stream","text":["2it [00:00, 13336.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Missing paper ids: 0\n","Added paper ids: 0\n"]}],"source":["import pandas as pd\n","df_all = pd.read_csv('data/dblp.csv')\n","\n","df_ml = df_all[df_all['keyword'].str.contains('statistics|machine learning|deep learning|artificial intelligence|neural networks', case=False, na=False)]\n","\n","import pandas as pd\n","from tqdm import tqdm\n","missing_paper_n = 99999999\n","id_all = df_all['id'].values\n","selected_paper_ids = df_ml['id'].values\n","\n","references = []\n","df_added = df_ml\n","\n","while missing_paper_n > 0:\n","    references = []\n","    missing_paper_ids = []\n","\n","    for i in tqdm(df_added.iterrows()):\n","        row = i[1]\n","        for ref in str(row['references']).split(';'):\n","            if ref == 'nan' or not ref or ref == 'None':\n","                continue\n","            ref = int(ref)\n","            references.append(ref)\n"," \n","    references = list(set(references))\n","\n","    len(references)\n","\n","    missing_paper_ids = list(set(references) - set(selected_paper_ids))\n","    selected_paper_ids = list(set(selected_paper_ids).union(set(references)))\n","    missing_paper_n = len(missing_paper_ids)\n","    # add the missing papers to the dataframe\n","    df_ml = pd.concat([df_ml, df_all[df_all['id'].isin(missing_paper_ids)]])\n","    df_added = df_all[df_all['id'].isin(missing_paper_ids)]\n","\n","    print('Missing paper ids:', len(missing_paper_ids))\n","    print('Added paper ids:', len(df_added))\n","\n","\n","len(df_ml)\n","\n","df_ml.to_csv('data/dblp_ml.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Extracting additional information\n","For each selected paper, we extract specific details such as the paperâ€™s ID, title, year, authors, and venue name.\n","Then, we save this information to a new csv file that will be useful for analysis.\n","Since the graph itself is very heavy in memory, it's better to keep this information separated to avoid adding more weight to the Graph object. With a join using the node ID we'll be able to retrieve additional information.\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2000000:1335.73s Finished processing 4894081 papers.\n"]}],"source":["import ijson\n","import time\n","import csv\n","import numpy as np\n","from tqdm import tqdm\n","import pandas as pd\n","\n","id_ml_papers = df_ml['id'].values\n","\n","\n","start = time.process_time()\n","\n","PAPER = []\n","Author = []\n","count = 0\n","\n","with open('data/dblp.v12.json', \"rb\") as f, open(\"data/papers.csv\", \"w\", newline=\"\") as csvfile:\n","    fieldnames = ['id', 'title', 'year', 'authors', 'abstract', 'venue_name']\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    for i, element in enumerate(ijson.items(f, \"item\")):\n","        paper = {}\n","        count += 1\n","        if element['id'] not in id_ml_papers:\n","            continue\n","        paper['id'] = element['id']\n","        paper['title'] = element['title']\n","\n","        year = element.get('year')\n","        paper['year'] = year if year else np.nan\n","\n","        author = element.get('authors')\n","        if author:\n","            author_names = [str(a.get('name', np.nan)) for a in author]\n","            paper['authors'] = ';'.join(author_names)\n","        else:\n","            paper['authors'] = np.nan\n","\n","        venue = element.get('venue', {}).get('raw')\n","        paper['venue_name'] = venue if venue else np.nan\n","\n","        writer.writerow(paper)\n","\n","        if count % 1000000 == 0:\n","            print(f\"{count}:{round((time.process_time() - start), 2)}s \", end=\"\")\n","\n","print(f\"Finished processing {count} papers.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.1.-1"}},"nbformat":4,"nbformat_minor":4}
